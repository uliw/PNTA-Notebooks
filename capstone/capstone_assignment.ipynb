{"cells":[{"cell_type":"markdown","metadata":{},"source":"Capstone Assignment\n===================\n\n**Author:** Ulrich G. Wortmann\n\n"},{"cell_type":"markdown","metadata":{},"source":["## Goal\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Today's assignment has two parts:\n\n1.  Write a program (notebook) that can do regression analysis on arbitrary CSV files. You can re-use many parts of last week's assignment, but you likely have to modify it. The user of your program will only specify:\n\n2.  Use the new code to solve the calibration assignment below, i.e., you submit a notebook with your code and the results obtained with the calibration data below.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Part 1 - Preparing the code\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In the following, we want to create a notebook that can be used with any given dataset to produce a linear regression analysis. The point of the notebook is to make this analysis as comfortable as possible, so we will add a few bells and whistles. The notebook should have the following sequence \n\n1.  A cell where you specify the CSV file name, and the figure file name, which columns  to use, and the confidence level\n\n2.  Starting with the next cell, all code actions should only depend on the variables defined in the first cell. Continue with reading the data from the CSV file, and mapping the data frame columns to generic variables (e.g., `X` and `Y`, see below). Add a statement to sort the data frame in ascending order by \"X\". The sorting is needed because otherwise, some statistical tests may fail.\n\n3.  A cell where you use the Shapiro-Wilk test to test if the X and the Y data are normal distributed or not.\n    -   If the test fails, your code will apply a log transform to the variable that needs it,\n    -   Then it will apply the test again. \n        -   If the test fails again, produce a histogram plot and end your program with an error message telling the user that neither the data nor the log-transformed data follows a normal distribution.\n        -   If the test succeeds without using a log transform, proceed to the next cell\n        -   If the test succeeds after applying a log transform,  add the log-transformed data to the dataframe, save the dataframe as CSV file, print a warning that states that  X and/or Y have been log-transformed, and then proceed to the next cell\n\n4.  Print the Pearson correlation coefficient using a suitable print statement (i.e., don't just print a number!)\n5.  Perform the regression analysis for `Y~X` and print the resulting parameters\n6.  Extract all data needed to make a regression plot that shows the regression parameters, and the confidence intervals for the model and the predictions. In addition, also extract the residuals from the results object like this:\n    \n        residuals: pd.Series = results.resid\n\n7.  Create a graph with 1 column and two rows. The first plot will be the usual regression plot, whereas the second graph should plot the residuals versus the x-axis data. The graph should be 120 dpi, 6 by 8 inches.  The plot must use the CSV file column headers as axis labels (modify accordingly if you used a log transform!). Place your annotation in the upper left corner if you have a positive correlation, and in the lower left corner if your data shows a negative correlation. See this example:\n\n![img](./sed.png)\n\n8.  Note that you should derive the text position for the regression parameters automatically. You can retrieve the axis dimensions as `ax.get_xlim()` and `ax.get_ylim()`.  Consider whether your correlation coefficient is positive or negative when you compute the text position.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Column names\n\n"]},{"cell_type":"markdown","metadata":{},"source":["To keep our code universal, and to avoid potential problems with the statsmodel library, it is best if we use standardized column names, rather than \"Babies 10<sup>3</sup>/yr\" which would fail with the statsmodel formula interface. It is thus best if you copy your data in generic variables (i.e., `X` and `Y`) this way there is no need to change column names on the dataframe.\n\n\\*\\* Rules\n\n-   At this point, you have seen many examples of well-written code. Follow these examples in building your own code\n-   Provide a header with an authorship and purpose statement\n-   When reading the CSV file, use the pathlib library to ensure that the file exists\n-   Use comments\n-   Use type hints\n-   If you want to, use functions, but for this exercise, it is not needed\n-   All figures use the ggplot style\n-   Develop your code step by step!\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Testing your code\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Run the notebook with the storks and sulfate reduction data, to make sure it functions as intended. Note: I do not need to see these tests. This is purely for your benefit.\n\nPay attention to the residual plot. For well-behaved data, the residuals should be evenly distributed, if they are not (e.g., in the stork data), or they show a clear pattern, it is a strong indication that your analysis is not valid.  Test your code with the following datasets:\n\n-   `nd_positive.csv`  positive correlation\n-   `nd_negative.csv` negative correlation\n-   `x_log.csv` this data requires a log transform in x\n-   `y_log.csv` this data requires a log transform in y\n-   `x_log_y_log.csv` this data requires a log-transform in x and y\n-   `x_skewed.csv` this data is highly correlated but x is not normal distributed\n-   `y_skewed.csv` this data is highly correlated but y is not normal distributed\n-   `x_and_y_skewed.csv` this data is highly correlated but x and y are not normal distributed\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Part 2: Let's put your code to use\n\n"]},{"cell_type":"markdown","metadata":{},"source":["To quote chatgpt: \n\n> Measurement uncertainty refers to the doubt or range of possible values associated with a measurement result. It accounts for factors such as variation in measurement conditions, instruments, and human factors, and is typically expressed as an interval within which the true value is believed to lie with a certain level of confidence.Moreover, with measured data, there are two things to consider: \n\n-   The precision of the measurement, i.e., if you re-measure the same sample again and again, how much variation do you get? This number can be very small but does not imply that your measurement is correct (i.e., you can measure a wrong result with high precision).\n-   The accuracy: This value reflects how close your value matches the true value. Accuracy typically has a much larger uncertainty than precision.\n\nIn the following, we will use your code to evaluate the accuracy (or error range) of some real-life data. As part of my research in the International Ocean Drilling Program (IODP), I occasionally go on expeditions with the JR Joides Resolution\n\n![img](Joides.jpg)\nThe ship takes about 24 scientists, 24 technicians, 23 drillers, and about 22 seamen. Expeditions are typically two months in length. While the scientists have cruise-related research objectives, their duties during the cruise are taken up by routine measurements describing the core materials. This includes geomagnetic, sedimentological, chemical, biological, mineralogical, and structural data.\n\nI participated twice and was part of the team documenting the inorganic chemistry of the water that is trapped between sediment grains - the so-called interstitial water (IW).\n\nThe chemistry of the interstitial water is monitored for many ionic species, among them Ammonium (NH<sub>4</sub><sup>+</sup>). Ammonium is measured on a photo spectrometer, which measures how much light passes through a sample container at a specified wavelength. \n\nThe photo spectrometer readings are very precise, but to get true concentration values, it requires manual calibration. In the first step, you have to mix standards. In the second step, you measure those standards to create a calibration function. The calibration function allows you to relate the light absorbance to the NH<sub>4</sub><sup>+</sup> concentration in your standards. In other words, we build a linear model to predict the NH<sub>4</sub><sup>+</sup> concentrations based on the absorbance values we get from the photo spectrometer.  The whole process is a bit of an art, but after practicing it for three days, I got this:\n\n![img](ammonium_311.png)\n\nBelow, you will find calibration data for my first expedition in 1998. I was a total noob, I did not practice, and likely I was not careful enough when I prepared my calibration standards. Alas, my lab journal lacks enough detail to understand why it was so bad.\n\nThe data for the above table is in the file `ammonia-1998.csv`.  Next, use your shiny new code to perform a regression analysis where you treat the absorbance data as the independent variable and the NH<sub>4</sub><sup>+</sup> concentration as the dependent variable. The regression analysis should be performed at a 99% confidence level. Since there are only a few data points, the Shapiro-Wilk test may fail. Maybe you need to add an override option to your code.\n\nDescribe in your own words the measurement uncertainty  (i.e., the prediction interval)  for the NH<sub>4</sub><sup>+</sup> concentration in units &mu; mol/l for an absorbance reading of 600. Write a few words about how this compares to the uncertainty you get for `ammonia-2005.csv` .\n\nNote the structure in the residuals for the 2005 data. This strongly suggests that a linear model is not suitable for this data and that the measurement error could be further reduced by a polynomial fit.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Marking Scheme\n\n"]},{"cell_type":"markdown","metadata":{},"source":["There are no partial points. Total Points: 24 pts\n\n-   Correct use of type hints: 1 pts\n-   Correct use of comments and doc strings 1pt\n-   Correct results for r<sup>2</sup>, p, and the regression equation: 3 pts\n-   Correct graph labels: 1 pt\n-   Display of the confidence interval for the regression model: 1 pt\n-   Display of the confidence interval for the predictions of the\n    model: 1 pt\n-   Correct value for the measurement uncertainty 6 pts\n-   Code calculates useful values for the regression text coordinates 2 pts\n-   Code correctly replaces column headers 2 pts\n-   Code handles log-transforms correctly 2pts\n-   Code adjusts axis label(s) depending in case a log-transform was necessary 2 pts\n-   Code plots the file name as plot title 1 pt\n-   Correct graph dimensions 1 pt\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Submission Instructions\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Create a new (or copy and existing) notebook in your `submissions`\nfolder before editing it. Otherwise, your edits may be overwritten the\nnext time you log into syzygy. Please name your copy `assignment-name-firstname-lastname` \n\n-   Replace the `assignment-name` with the name of the assignment\n    (i.e., the filename of the respective Jupyter Notebook)\n-   `firstname-lastname` with your own name.\n\nNote: If the notebook contains images, you must also copy the image files!\n\nYour notebook/pdf must start with the following lines :\n\n**ESS245: Assignment Title**\n\n**Date:**\n\n**First Name:**\n\n**Last Name:**\n\n**Student: Id**\n\nBefore submitting your assignment:\n\n-   Check the marking scheme and ensure you have covered all requirements.\n-   Re-read the learning outcomes and verify that you are comfortable\n    with each concept. If not, please speak up on the discussion board\n    and ask for further clarification. I can guarantee that if you feel\n    uncertain about a concept, at least half the class will be in the\n    same boat. So don't be shy!\n\nTo submit your assignment, you need to download it as `ipynb` notebook\nformat **and** `pdf` format. **To export your notebook as pdf\nuse your browser's print function (`Ctrl-P`) and then select**\n`Save as pdf`.  In the past, this worked best with Chrome or Firefox.\n\n Please submit **both files** on Quercus. Note that the pdf\nexport can fail if your file contains invalid markup/python code. So\nyou need to check that the pdf export is complete and does not miss\nany sections. If you have export problems, don't hesitate to contact\nthe course instructor directly.\n\nNotebooks typically have empty code cells in which you must enter\npython code. Please use the respective cell below each question, or\ncreate a python cell where necessary. Add text cells to enter your\nanswers where appropriate. Your responses will only count if the code\nexecutes without error. It is thus recommended to run your solutions\nbefore submitting the assignment.\n\n**Note: Unless specifically requested, do not type your answers by**\n**hand. Instead, write code that produces the answer. Your pdf file**\n**should show the code and the results of the code execution.**\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}