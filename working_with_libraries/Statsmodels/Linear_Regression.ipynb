{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Linear Regression\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Causation versus Correlation\n\n"]},{"cell_type":"markdown","metadata":{},"source":[" Back in\n my home country, and before the hippy movement changed our culture,\n kids, who were curious about where the babies come from, were told that\n they are brought by the stork (a large bird, see\n Fig.[fig:storksa](#fig:storksa)). Storks were indeed a common sight in rural\n areas, and large enough to sell this story to a 3-year-old.\n\n![img](Ringed_white_stork.png \"The Stork. Image by Soloneying, from ![img](https://commons.wikimedia.org/wiki/File:Ringed_white_stork.jpg) Downloaded Nov 22<sup>nd</sup> 2019.\")\n\nSadly, as grown-up scientists with a penchant for critical thinking,\nwe want to know if there is data to support this idea. Specifically,\nwe should see a good correlation between the number of storks and the\nnumber of babies. Low and behold, these two variables actually\ncorrelate in a statistically significant way. Countries with larger\nstork populations have higher birthrates.  Since both variables\nincrease together, this is called a positive correlation. See\nFig. [4](#org408ee76)\n\n![img](./storks.png \"The birthrate and the number of stork pairs correlate in a statistical significant way. This analysis suggests that each stork pair delivers about 29 human babies, and that about 225 thousand babies were born otherwise. Data after Matthews 2000.\")\n\nNow, does this prove that the storks deliver the babies? Obviously (or so we\nthink) not. Just because two observable quantities correlate does in no way\nimply that one is the cause of the other. The more likely explanation is that\nboth variables are affected by a common process (i.e., industrialization).\n\nIt is a common mistake to confuse correlation with causation. Another\ngood example is to correlate drinking with heart attacks. This surely\nwill correlate, but the story is more complex. Are there, e.g.,\npatterns like drinkers tend to do less exercise than non-drinkers? So\neven if you have a good hypothesis why two variables are correlated,\nthe correlation itself proves nothing. \n\nIrrespective of a causal relationship, we can express the correlation\nbetween two datasets as the Pearson Product Moment Correlation\nCoefficient (PPMC, typically called `r`) to describe the strength and direction of the\nrelationship between two variables.   The PPMCC (lower case r) varies\nbetween +1 (perfect positive correlation) and -1 (perfect negative or\ninverse correlation). Correlations are described as weak if they are\nbetween +0.3 and -0.3, strong if they greater than +0.7 or less than\n-0.7”. Note, that correlation analysis makes no assumptions about the\nfunctional form between y (dependent variable) and x (independent\nvariable). IN other words, the PPMC says nothing about whether the\ncorrelation is linear, logarithmic, exponential etc.\n\nWe can use the `corr()` method of the pandas series object to calculate the PPMCC\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd  # inport pandas as pd\nimport pathlib as pl\n\nfn: str = \"storks_vs_birth_rate.csv\"  # file name\ncwd: pl.Path = pl.Path.cwd()\nfqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\n\n# this little piece of code could have saved me 20 minutes\nif not fqfn.exists():  # check if the file is actually there\n    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n\ndf: pd.DataFrame = pd.read_csv(fn)  # read data\ndf.columns = [\"Babies\", \"Storks\"]  # replace colum names \nb: pd.Series = df[\"Babies\"]\ns: pd.Series = df[\"Storks\"]\n\nprint(f\" r = {s.corr(b):.2f}\")"]},{"cell_type":"markdown","metadata":{},"source":["So we confirm that the number of babies and storks correlate.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Understanding the results of a linear regression analysis\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Linear regression analysis takes correlation analysis one step further\nand determines how well a linear function (e.g., a straight line) can\ndescribe the relation between x and y.  The equation of a straight\nline `y = mx +b` is fitted to the scatter of data by changing `a` and\n`m` in such a way that the difference between the measured data and\nthe model prediction is minimized.\n\nThe Coefficient of Determination, or r<sup>2</sup> expresses how well\na sloping straight line can explain the correlation between the \ndependent (y) and a single independent (x) variable. In the above\nfigure, r<sup>2</sup> = 0.38, which means that 38% of the new-born babies could\nbe explained by a linear correlation with the number of storks.\n\nIn many cases, more than one independent variable is needed to explain\nthe scatter in the data. In this case Multiple Linear Regression is\nused where y = x1 + x2 + x3….xn + b. From this analysis\n(i.e. simultaneous solving for a system of linear equations – remember\nyour Linear Algebra course!) the Multiple Coefficient of Determination\n(R<sup>2</sup>) is used to express the amount of explained variation in y by a\ncombination of independent variables. By default, many programs use\nthe R<sup>2</sup> number even when there is only one independent variable. This\ncan be misleading as capital R<sup>2</sup> should be reserved for analyses that\ninvolve multiple independent variables.\n\nFrom a user perspective, we are interested to understand how good the\nmodel actually is, and how to interpret the key indicators of a given\nregression model:\n\n-   **r<sup>2</sup>:** or coefficient of determination. \\index{linear\n    regression!rsquare} \\index{linear regression!coefficient of\n    determination} This value is in the range from zero to one and\n    expresses how much of the observed variance \\index{linear\n    regression!variance} in the data is explained by the regression\n    model. So a value of r<sup>2</sup>=0.7 indicates that 70% of the variance is\n    explained by the model, and that 30% of the variance is explained\n    by other processes which are not captured by the linear model\n    (e.g., measurements errors, or some non-linear effect affecting `x`\n    and `y`). In Fig. [BROKEN LINK: fig:storks] 38% of the variance in the birthrate\n    can be explained by the increase in stork pairs.\n-   **p:** When you do a linear regression, you basically state the\n    hypothesis that `y` depends on `x` and that they are linked by a\n    linear equation. If you test a hypothesis, you however also have to\n    test the so-called **null-hypothesis**, which in this case would\n    \\index{linear regression!null hypothesis} state that `y` is\n    \\index{linear regression!p-value} unrelated to `x`. The p-value\n    expresses the likelihood that the null-hypothesis is true. So a\n    p-value of 0.1 indicates a 10% chance that your data does not\n    correlate. A p-value of 0.01, indicates a 1% chance that your data\n    is not correlated. Typically, we can reject the null-hypothesis if\n    `p < 0.05`, in other words, we are 95% sure the null hypothesis is\n    wrong. In Fig. [BROKEN LINK: fig:storks], we are 99.2% sure the null hypothesis is\n    wrong. Note that there is not always a simple relationship between\n    r<sup>2</sup> and p.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### The statsmodel library\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\nPythons success rests to a considerable degree on the myriad of third\nparty libraries which, unlike matlab, are typically free to use. In\nthe following, we will use the \"statsmodel\" library, but there are\nplenty of other statistical libraries we could use as well. \n\nThe statsmodel library provides different interfaces. Here we will use\nthe formula interface, which is similar to the R-formula\nsyntax. However, not all statsmodel functions are available through\nthis interface (yet?). First, we import the needed libraries:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd  # import pandas as pd\nimport pathlib as pl\n\n# define the file and sheetname we want to read. Note that the file\n# has to be present in the local working directory!\nfn: str = \"storks_vs_birth_rate.csv\"  # file name\ncwd: pl.Path = pl.Path.cwd()\nfqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\nif not fqfn.exists():  # check if the file is actually there\n    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n\ndf: pd.DataFrame = pd.read_csv(fn)  # read data\ndf.columns = [\"Babies\", \"Storks\"]  # replace colum names\ndf.head()  # test that all went well"]},{"cell_type":"markdown","metadata":{},"source":["Before we perform a regression analysis, me must test if the  data follows a normal\ndistribution. There are a variety of tests to check for normality, but\nhere we will simply use a histogram plot which either shows a bell\ncurve distribution or not \n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n\nfig: plt.Figure\nax1: plt.Axes\nax2: plt.Axes\n\nfig, [ax1, ax2] = plt.subplots(nrows=2, ncols=1)  #\nfig.set_size_inches(6, 9)\nax1.hist(\n    df.iloc[:, 0],\n)\nax2.hist(df.iloc[:, 1])\nax1.set_title(\"Babies\")\nax2.set_title(\"Storks\")\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["As you can see from the histogram, our data shows anything but a\nnormal distribution! We will use it anyway since it is a fun\ndataset. However, the above test is crucial if you ever want to do a\nreal regression analysis!\n\nFor the regression model, we want to analyze whether the number of\nstorks predicts the number of babies. In other words, does the birth\nrate depend on the number of storks? For this, we need to define a\nstatistical model, and test whether the model predictions will fit the\ndata:\n\n-   The gory details of this procedure are beyond the scope of this\n    course - if you have not yet taken a stats class, I do recommend\n    doing so!\n-   There are many ways of doing this. Here we use an approach which\n    is common in `R`\n\nAs of November 2021, there is no type hinting support for the statsmodel library.  However, in order to  distinguish between the various statsmodel object types, I use the following hints:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import statsmodels.formula.api as smf\n\nmodel: smf.ols  # ordinary least square model object\nresults: smf.ols.fit  # smf.ols.model results object"]},{"cell_type":"markdown","metadata":{},"source":["After importing the data, we now create a statistical model on line 16\nin the code below.  Pay attention how the model is specified with the\nformula `\"Babies ~ Storks`, which states that the number of Babies\nshould depend on the number of storks. These names must correspond to\nthe variable names in the dataframe `df`!\n\nOnce the model is defined, we request to fit the model against the data (line 17)\nThe results of the `fit()` method will be stored in the `results` variable.\nLine 18, then  invokes the\n`summary()` method of the results object.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import statsmodels.formula.api as smf\nimport pandas as pd  # import pandas as pd\nimport pathlib as pl\n\n# define the file and sheetname we want to read. Note that the file\n# has to be present in the local working directory!\nfn: str = \"storks_vs_birth_rate.csv\"  # file name\ncwd: pl.Path = pl.Path.cwd()\nfqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\nif not fqfn.exists():  # check if the file is actually there\n    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n\ndf: pd.DataFrame = pd.read_csv(fn)  # read data\ndf.columns = [\"Babies\", \"Storks\"]  # replace colum names\n\nmodel: smf.ols = smf.ols(formula=\"Babies ~ Storks\", data=df)\nresults: model.fit = model.fit()  # fit the model to the data\nprint(results.summary())  # print the results of the analysis"]},{"cell_type":"markdown","metadata":{},"source":["Plenty of information here, probably more than you asked for. \nLet's tease out the important ones:\n\n-   The first line states that `Babies` is the dependent variable. This\n    is useful and will help you to catch errors in your model\n    definition.\n-   The second lines confirms that this is an ordinary least squares model\n-   Then, there are also a couple of warnings, indicating that your\n    data quality may be less than excellent. But we knew this already\n    from testing whether the data is normal distributed or not.\n\nIf you compare the output with Figure [fig:storks](#fig:storks), you can see that\nr<sup>2</sup> value is called \"R-squared\", the p-value is called \"Prob\n(F-statistic)\", the y-intercept is the first value in the \"Intercept\"\nrow, the slope is the first value in the \"Storks\" row. You can also\nextract these parameters from the model results object like this:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# retrieve values from the model results\nslope   :float = results.params[1]  # the slope\ny0      :float = results.params[0]  # the y-intercept\nrsquare :float = results.rsquared   # rsquare\npvalue  :float = results.pvalues[1] # the pvalue"]},{"cell_type":"markdown","metadata":{},"source":["Using these parameters, we can now calculate the regression line using\nthe `predict()` method of the model results object and add it to the\ngraph.  We will explore how to create the confidence interval in the\nnext module.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### References\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Robert Matthews, Storks Devilver Babies (p = 0.008), Teaching\n    Statistics 22:2, p 36-38, 2000,\n    [https://doi.org/10.1111/1467-9639.00013](https://doi.org/10.1111/1467-9639.00013)\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}