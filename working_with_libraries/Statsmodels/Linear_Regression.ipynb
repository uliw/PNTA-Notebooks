{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Linear Regression\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Causation versus Correlation\n\n"]},{"cell_type":"markdown","metadata":{},"source":[" Back in\n my home country, and before the hippy movement changed our culture,\n kids, who were curious about where the babies come from, were told that\n they are brought by the stork (a large bird, see\n Fig.[fig:storksa](#fig:storksa)). Storks were indeed a common sight in rural\n areas, and large enough to sell this story to a 3-year-old.\n\n![img](Ringed_white_stork.png \"The Stork. Image by Soloneying, from ![img](https://commons.wikimedia.org/wiki/File:Ringed_white_stork.jpg) Downloaded Nov 22<sup>nd</sup> 2019.\")\n\nSadly, as grown-up scientists with a penchant for critical thinking,\nwe want to know if there is data to support this idea. Specifically,\nwe should see a good correlation between the number of storks and the\nnumber of babies. Low and behold, these two variables actually\ncorrelate in a statistically significant way. Countries with larger\nstork populations have higher birthrates.  Since both variables\nincrease together, this is called a positive correlation. See\nFig. [4](#org0baeb06)\n\n![img](./stork_new.png \"The birthrate and the number of stork pairs correlate in a statistical significant way. This analysis suggests that each stork pair delivers about 29 human babies, and that about 225 thousand babies were born otherwise. Data after Matthews 2000.\")\n\nNow, does this prove that the storks deliver the babies? Obviously (or so we\nthink) not. Just because two observable quantities correlate does in no way\nimply that one is the cause of the other. The more likely explanation is that\nboth variables are affected by a common process (i.e., industrialization).\n\nIt is a common mistake to confuse correlation with causation. Another\ngood example is to correlate drinking with heart attacks. This surely\nwill correlate, but the story is more complex. Are there, e.g.,\npatterns like drinkers tend to do less exercise than non-drinkers? So\neven if you have a good hypothesis why two variables are correlated,\nthe correlation itself proves nothing. \n\nIrrespective of a causal relationship, we can express the correlation\nbetween two datasets as the Pearson Product Moment Correlation\nCoefficient (PPMC, typically called `r`) to describe the strength and direction of the\nrelationship between two variables.   The PPMCC (lower case r) varies\nbetween +1 (perfect positive correlation) and -1 (perfect negative or\ninverse correlation). Correlations are described as weak if they are\nbetween +0.3 and -0.3, strong if they greater than +0.7 or less than\n-0.7”. Note, that correlation analysis makes no assumptions about the\nfunctional form between y (dependent variable) and x (independent\nvariable). IN other words, the PPMC says nothing about whether the\ncorrelation is linear, logarithmic, exponential etc.\n\nWe can use the `corr()` method of the pandas series object to calculate the PPMCC\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd  # inport pandas as pd\nimport pathlib as pl\n\nfn: str = \"storks_vs_birth_rate.csv\"  # file name\ncwd: pl.Path = pl.Path.cwd()\nfqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\n\n# this little piece of code could have saved me 20 minutes\nif not fqfn.exists():  # check if the file is actually there\n    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n\ndf: pd.DataFrame = pd.read_csv(fn)  # read data\ndf.columns = [\"Babies\", \"Storks\"]  # replace colum names \nb: pd.Series = df[\"Babies\"]\ns: pd.Series = df[\"Storks\"]\n\nprint(f\" r = {s.corr(b):.2f}\")"]},{"cell_type":"markdown","metadata":{},"source":["So we confirm that the number of babies and storks correlate.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Understanding the results of a linear regression analysis\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Linear regression analysis takes correlation analysis one step further\nand determines how well a linear function (e.g., a straight line) can\ndescribe the relation between x and y.  The equation of a straight\nline `y = mx +b` is fitted to the scatter of data by changing `a` and\n`m` in such a way that the difference between the measured data and\nthe model prediction is minimized.\n\nThe Coefficient of Determination, or r<sup>2</sup> expresses how well\na sloping straight line can explain the correlation between the \ndependent (y) and a single independent (x) variable. In the above\nfigure, r<sup>2</sup> = 0.38, which means that 38% of the new-born babies could\nbe explained by a linear correlation with the number of storks.\n\nIn many cases, more than one independent variable is needed to explain\nthe scatter in the data. In this case Multiple Linear Regression is\nused where y = x1 + x2 + x3….xn + b. From this analysis\n(i.e. simultaneous solving for a system of linear equations – remember\nyour Linear Algebra course!) the Multiple Coefficient of Determination\n(R<sup>2</sup>) is used to express the amount of explained variation in y by a\ncombination of independent variables. By default, many programs use\nthe R<sup>2</sup> number even when there is only one independent variable. This\ncan be misleading as capital R<sup>2</sup> should be reserved for analyses that\ninvolve multiple independent variables.\n\nFrom a user perspective, we are interested to understand how good the\nmodel actually is, and how to interpret the key indicators of a given\nregression model:\n\n-   **r<sup>2</sup>:** or coefficient of determination. \\index{linear\n    regression!rsquare} \\index{linear regression!coefficient of\n    determination} This value is in the range from zero to one and\n    expresses how much of the observed variance \\index{linear\n    regression!variance} in the data is explained by the regression\n    model. So a value of r<sup>2</sup>=0.7 indicates that 70% of the variance is\n    explained by the model, and that 30% of the variance is explained\n    by other processes which are not captured by the linear model\n    (e.g., measurements errors, or some non-linear effect affecting `x`\n    and `y`). In Fig. [BROKEN LINK: fig:storks] 38% of the variance in the birthrate\n    can be explained by the increase in stork pairs.\n-   **p:** When you do a linear regression, you basically state the\n    hypothesis that `y` depends on `x` and that they are linked by a\n    linear equation. If you test a hypothesis, you however also have to\n    test the so-called **null-hypothesis**, which in this case would\n    \\index{linear regression!null hypothesis} state that `y` is\n    \\index{linear regression!p-value} unrelated to `x`. The p-value\n    expresses the likelihood that the null-hypothesis is true. So a\n    p-value of 0.1 indicates a 10% chance that your data does not\n    correlate. A p-value of 0.01, indicates a 1% chance that your data\n    is not correlated. Typically, we can reject the null-hypothesis if\n    `p < 0.05`, in other words, we are 95% sure the null hypothesis is\n    wrong. In Fig. [BROKEN LINK: fig:storks], we are 99.2% sure the null hypothesis is\n    wrong. Note that there is not always a simple relationship between\n    r<sup>2</sup> and p.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### The statsmodel library\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\nPythons success rests to a considerable degree on the myriad of third\nparty libraries which, unlike matlab, are typically free to use. In\nthe following, we will use the \"statsmodel\" library, but there are\nplenty of other statistical libraries we could use as well. \n\nThe statsmodel library provides different interfaces. Here we will use\nthe formula interface, which is similar to the R-formula\nsyntax. However, not all statsmodel functions are available through\nthis interface (yet?). First, we import the needed libraries:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd  # import pandas as pd\nimport pathlib as pl\n\n# define the file and sheetname we want to read. Note that the file\n# has to be present in the local working directory!\nfn: str = \"storks_vs_birth_rate.csv\"  # file name\ncwd: pl.Path = pl.Path.cwd()\nfqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\nif not fqfn.exists():  # check if the file is actually there\n    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n\ndf: pd.DataFrame = pd.read_csv(fn)  # read data\ndf.columns = [\"Babies\", \"Storks\"]  # replace colum names\ndf.head()  # test that all went well"]},{"cell_type":"markdown","metadata":{},"source":["Before we perform a regression analysis, me must test if the  data follows a normal\ndistribution. There are a variety of tests to check for normality, but\nhere we will simply use a histogram plot which either shows a bell\ncurve distribution or not \n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n\nfig: plt.Figure\nax1: plt.Axes\nax2: plt.Axes\n\nfig, [ax1, ax2] = plt.subplots(nrows=2, ncols=1)  #\nfig.set_size_inches(6, 9)\nax1.hist(\n    df.iloc[:, 0],\n)\nax2.hist(df.iloc[:, 1])\nax1.set_title(\"Babies\")\nax2.set_title(\"Storks\")\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["As you can see from the histogram, our data shows anything but a\nnormal distribution! We will use it anyway since it is a fun\ndataset. However, the above test is crucial if you ever want to do a\nreal regression analysis!\n\nFor the regression model, we want to analyze whether the number of\nstorks predicts the number of babies. In other words, does the birth\nrate depend on the number of storks? For this, we need to define a\nstatistical model, and test whether the model predictions will fit the\ndata:\n\n-   The gory details of this procedure are beyond the scope of this\n    course - if you have not yet taken a stats class, I do recommend\n    doing so!\n-   There are many ways of doing this. Here we use an approach which\n    is common in `R`\n\nAs of November 2021, there is no type hinting support for the statsmodel library.  However, in order to  distinguish between the various statsmodel object types, I use the following hints:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import statsmodels.formula.api as smf\n\nmodel: smf.ols  # ordinary least square model object\nresults: smf.ols.fit  # smf.ols.model results object"]},{"cell_type":"markdown","metadata":{},"source":["After importing the data, we now create a statistical model on line 16\nin the code below.  Pay attention how the model is specified with the\nformula `\"Babies ~ Storks`, which states that the number of Babies\nshould depend on the number of storks. These names must correspond to\nthe variable names in the dataframe `df`!\n\nOnce the model is defined, we request to fit the model against the data (line 17)\nThe results of the `fit()` method will be stored in the `results` variable.\nLine 18, then  invokes the\n`summary()` method of the results object.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import statsmodels.formula.api as smf\nimport pandas as pd  # import pandas as pd\nimport pathlib as pl\n\n# define the file and sheetname we want to read. Note that the file\n# has to be present in the local working directory!\nfn: str = \"storks_vs_birth_rate.csv\"  # file name\ncwd: pl.Path = pl.Path.cwd()\nfqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\nif not fqfn.exists():  # check if the file is actually there\n    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n\ndf: pd.DataFrame = pd.read_csv(fn)  # read data\ndf.columns = [\"Babies\", \"Storks\"]  # replace colum names\n\nmodel: smf.ols = smf.ols(formula=\"Babies ~ Storks\", data=df)\nresults: model.fit = model.fit()  # fit the model to the data\nprint(results.summary())  # print the results of the analysis"]},{"cell_type":"markdown","metadata":{},"source":["Plenty of information here, probably more than you asked for. \nLet's tease out the important ones:\n\n-   The first line states that `Babies` is the dependent variable. This\n    is useful and will help you to catch errors in your model\n    definition.\n-   The second lines confirms that this is an ordinary least squares model\n-   Then, there are also a couple of warnings, indicating that your\n    data quality may be less than excellent. But we knew this already\n    from testing whether the data is normal distributed or not.\n\nIf you compare the output with Figure [fig:storks](#fig:storks), you can see that\nr<sup>2</sup> value is called \"R-squared\", the p-value is called \"Prob\n(F-statistic)\", the y-intercept is the first value in the \"Intercept\"\nrow, the slope is the first value in the \"Storks\" row. You can also\nextract these parameters from the model results object like this:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# retrieve values from the model results\nslope   :float = results.params[1]  # the slope\ny0      :float = results.params[0]  # the y-intercept\nrsquare :float = results.rsquared   # rsquare\npvalue  :float = results.pvalues[1] # the pvalue"]},{"cell_type":"markdown","metadata":{},"source":["### Adding the regression line and confidence intervals\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The r<sup>2</sup> and p-value give us some indication of how good our regression\nmodel performs. However, we can add further information to our graph:\n\n-   The line which represents the regression model\n-   The confidence intervals that indicates the confidence we have in our\n    regression model.\n-   The confidence intervals that indicate the confidence we have in\n    the predictions we make based on our regression model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Accessing the confidence interval data\n\n"]},{"cell_type":"markdown","metadata":{},"source":["It is not easy to find out how to access these parameters in the\nstatslib library. So best to keep this code snippet in your template\ncollection. Bottom line is, we will use the `summary_table()` function\nprovided by the `statsmodels.stats.outliers_influence` module. We can\nthen feed the `results` object of the regression analysis to this\nfunction, and it will return all sorts of data (most of which we can\nignore). Note that the `alpha` keyword specifies the significance\nlevel for the confidence intervals we aim to retrieve. It is customary\nto provide this number as 1-significance.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from __future__ import annotations\nfrom statsmodels.stats.outliers_influence import summary_table\n\nsig: float = 0.05  # = 1 - sig > 0.95 = 95% significance\n\n# variable types for the return values from summary_table\nst: SimpleTable  # table with results that can be printed\ndata: np.ndarray  # calculated measures and statistics for the table\nss2: list[str]  # column_names for table (Note: rows of table are observations)\n\n# variable types for the confidence interval data\nmodel_ci_low: np.ndarray\nmodel_ci_upp: np.ndarray\npredict_mean_ci_low: np.ndarray\npredict_mean_ci_up: np.ndarray\n\nst, data, ss2 = summary_table(results, alpha=0.05)\n\n# extract the data for predicted values and confidence intervals\nfitted_values: np.ndarray = data[:, 2]  # the regression line\n\n# confidence intervals for the model\nmodel_ci_low, model_ci_upp = data[:, 4:6].T  #\n\n# confidence intervals for the predictions based on the model\npredict_mean_ci_low, predict_mean_ci_upp = data[:, 6:8].T"]},{"cell_type":"markdown","metadata":{},"source":["#### Plotting the confidence interval data\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The upper and lower confidence boundaries describe the upper and lower\nboundaries of an area. We can either plot these boundaries as a line plot, or as a shaded area.\nMatplotlib provides an easy method to shade the area between two lines:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Note this code will not run by itself. Use the example below\nax.fill_between(storks, model_ci_low, model_ci_up, alpha=0.1, color=\"C1\")"]},{"cell_type":"markdown","metadata":{},"source":["Note the use of the `alpha` keyword. This has nothing todo with the\nalpha which is used in statitics. Rather, it decsribes the\ntransparancy of the object you are drawing. If you set it to one (the\ndefault), the object will be fully opaque. If you set it to zero, it\nwill be fully transparent (so you won't see it).\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Creating the Stork Figure\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now let's put it all together. Note that when we draw the figure, it\nmatters whether we draw the confidencve intervalls first or\nlast. Change the order in the code below, to see the difference.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from __future__ import annotations\nimport pandas as pd  # inport pandas as pd\nimport matplotlib.pyplot as plt\nimport pathlib as pl\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.outliers_influence import summary_table\n\nfn: str = \"storks_vs_birth_rate.csv\"  # file name\ncwd: pl.Path = pl.Path.cwd()\nfqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\n\n# test if file is present\nif not fqfn.exists():  # check if the file is actually there\n    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n\n# read data\ndf: pd.DataFrame = pd.read_csv(fn)  # read data\ndf.columns = [\"Babies\", \"Storks\"]  # replace colum names\nstorks: pd.Series = df[\"Storks\"]\nbabies: pd.Series = df[\"Babies\"]\n\n\n# ------ create linear regression model ------\nmodel: smf.ols = smf.ols(formula=\"Babies ~ Storks\", data=df)\nresults: model.fit = model.fit()  # fit the model to the data\n\n\n# ------ extract model parameters\nslope: float = results.params[1]  # the slope\ny0: float = results.params[0]  # the y-intercept\nrsquare: float = results.rsquared  # rsquare\npvalue: float = results.pvalues[1]  # the pvalue\nds: str = (\n    f\"y = {y0:1.4f}+x*{slope:1.4f}\\n\" f\"$r^2$ = {rsquare:1.2f}\\n\" f\"p = {pvalue:1.4f}\"\n)\n\n\n# ------ extract confidence intervals ------\nsig: float = 0.05  # = 1 - sig > 0.95 = 95% significance\n\n# variable types for the return values from summary_table\nst: SimpleTable  # table with results that can be printed\ndata: np.ndarray  # calculated measures and statistics for the table\nss2: list[str]  # column_names for table (Note: rows of table are observations)\n\n# variable types for the confidence interval data\nmodel_ci_low: np.ndarray\nmodel_ci_up: np.ndarray\npredict_mean_ci_low: np.ndarray\npredict_mean_ci_up: np.ndarray\n\n# get data\nst, data, ss2 = summary_table(results, alpha=sig)\n\n# extract regression line\nfitted_values: np.ndarray = data[:, 2]\n\n# extract confidence intervals for the model\nmodel_ci_low, model_ci_up = data[:, 4:6].T  #\n\n# extract confidence intervals for the predictions\npredict_mean_ci_low, predict_mean_ci_up = data[:, 6:8].T\n\n\n# ------ create plot ------\nfig: plt.Figure  # this variable  will hold the canvas object\nax: plt.Axes  # this variable will hold the axis object\nfig, ax = plt.subplots()  # create canvas and axis objects\n\n# plot confidence intervals first\nax.fill_between(storks, predict_mean_ci_low, predict_mean_ci_up, alpha=0.1, color=\"C1\")\nax.fill_between(storks, model_ci_low, model_ci_up, alpha=0.2, color=\"C1\")\n\n# add data points\nax.scatter(storks, babies, color=\"C0\")\n\n# regression line\nax.plot(storks, fitted_values, color=\"C1\")\n\n# plot options and annotations\nplt.style.use(\"ggplot\")\nfig.set_size_inches(6, 4)\nfig.set_dpi(120)\n\nax.text(1000, 1750, ds, verticalalignment=\"top\")\nax.set_xlabel(\"Stork Pairs\")\nax.set_ylabel(\"Newborn Babies [$10^3$]\")\nfig.set_tight_layout(\"tight\")\nfig.savefig(\"stork_new.png\")\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The code above is quite lengthy, especially given the fact that\nyou can do a regression analysis with a few clicks in excel. On the\nother hand, if you re-arrange the code a a little bit, and use generic\nvariables, you can create a code template where you only specify a few\nkey-parameters at the beginning of the code, and then you can\ngenerate a much more meaningful regression analysis with a few keystrokes:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["\"\"\" Description:\n    Author:\n    Date:\n\"\"\"\n# ----------- third party library imports ------------------\nfrom __future__ import annotations\nimport pandas as pd  # inport pandas as pd\nimport matplotlib.pyplot as plt\nimport pathlib as pl\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.outliers_influence import summary_table\n\n# ----------- user serviceable pararameters\ndata_file: str = \"\"  # csv file name\nfigure_name: str = \"\"  # figure name\n\nx_var_loc: int = 0  # colum # in spreadsheet\ny_var_loc: int = 1  # colum # in spreadsheer\n\nx_axis_label: str = \"\"\ny_axis_label: str = \"\"\nsize_x: number = 6  # size in inches\nsize_y: number = 4  # size in inches\n\nconfidence_level: float = 0.05  # 1 - alpha in %\n\n# ----------- main program ---------------------------------\n# --- variable declarations\n\n# --- code starts here"]},{"cell_type":"markdown","metadata":{},"source":["### References\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Robert Matthews, Storks Devilver Babies (p = 0.008), Teaching\n    Statistics 22:2, p 36-38, 2000,\n    [https://doi.org/10.1111/1467-9639.00013](https://doi.org/10.1111/1467-9639.00013)\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}