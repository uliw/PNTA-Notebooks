{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causation versus Correlation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Back in\n",
    " my home country, and before the hippy movement changed our culture,\n",
    " kids, who were curious about where the babies come from, were told that\n",
    " they are brought by the stork (a large bird, see\n",
    " Fig.[fig:storksa](fig:storksa)). Storks were indeed a common sight in rural\n",
    " areas, and large enough to sell this story to a 3-year-old.\n",
    "\n",
    "![img](Ringed_white_stork.png \"The Stork. Image by Soloneying, from ![img](https://commons.wikimedia.org/wiki/File:Ringed_white_stork.jpg) Downloaded Nov 22<sup>nd</sup> 2019.\")\n",
    "\n",
    "Sadly, as grown-up scientists with a penchant for critical thinking,\n",
    "we want to know if there is data to support this idea. Specifically,\n",
    "we should see a good correlation between the number of storks and the\n",
    "number of babies. Low and behold, these two variables actually\n",
    "correlate in a statistically significant way. Countries with larger\n",
    "stork populations have higher birthrates.  Since both variables\n",
    "increase together, this is called a positive correlation. See\n",
    "Fig. [4](#org2df91f1)\n",
    "\n",
    "![img](./stork_new.png \"The birthrate and the number of stork pairs correlate in a statistically significant way. This analysis suggests that each stork pair delivers about 29 human babies, and that about 225 thousand babies were born otherwise. Data after Matthews 2000.\")\n",
    "\n",
    "Now, does this prove that the storks deliver the babies? Obviously (or so we\n",
    "think) not. Just because two observable quantities correlate does in no way\n",
    "imply that one is the cause of the other. The more likely explanation is that\n",
    "both variables are affected by a common process (i.e., industrialization).\n",
    "\n",
    "It is a common mistake to confuse correlation with causation. Another\n",
    "good example is to correlate drinking with heart attacks. This surely\n",
    "will correlate, but the story is more complex. Are there, e.g.,\n",
    "patterns like drinkers tend to do less exercise than non-drinkers? So\n",
    "even if you have a good hypothesis why two variables are correlated,\n",
    "the correlation itself proves nothing. \n",
    "\n",
    "Irrespective of a causal relationship, we can express the correlation\n",
    "between two datasets as the Pearson Product Moment Correlation\n",
    "Coefficient (PPMC, typically called `r`) to describe the strength and direction of the\n",
    "relationship between two variables.   The PPMCC (lower case r) varies\n",
    "between +1 (perfect positive correlation) and -1 (perfect negative or\n",
    "inverse correlation). Correlations are described as weak if they are\n",
    "between +0.3 and -0.3, strong if they are greater than +0.7 or less than\n",
    "-0.7”. Note, that correlation analysis makes no assumptions about the\n",
    "functional form between y (dependent variable) and x (independent\n",
    "variable). In other words, the PPMC says nothing about whether the\n",
    "correlation is linear, logarithmic, exponential etc.\n",
    "\n",
    "We can use the `corr()` method of the pandas series object to calculate the PPMCC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # inport pandas as pd\n",
    "import pathlib as pl\n",
    "\n",
    "fn: str = \"storks_vs_birth_rate.csv\"  # file name\n",
    "cwd: pl.Path = pl.Path.cwd()\n",
    "fqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\n",
    "\n",
    "# this little piece of code could have saved me 20 minutes\n",
    "if not fqfn.exists():  # check if the file is actually there\n",
    "    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n",
    "\n",
    "df: pd.DataFrame = pd.read_csv(fn)  # read data\n",
    "df.columns = [\"Babies\", \"Storks\"]  # replace colum names \n",
    "b: pd.Series = df[\"Babies\"]\n",
    "s: pd.Series = df[\"Storks\"]\n",
    "\n",
    "print(f\" r = {s.corr(b):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can confirm that the number of babies and storks correlate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the results of a linear regression analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression analysis takes correlation analysis one step further\n",
    "and determines how well a linear function (e.g., a straight line) can\n",
    "describe the relation between x and y.  The equation of a straight\n",
    "line `y = mx +b` is fitted to the scatter of data by changing `a` and\n",
    "`m` in such a way that the difference between the measured data and\n",
    "the model prediction is minimized.\n",
    "\n",
    "The Coefficient of Determination or r<sup>2</sup> expresses how well\n",
    "a sloping straight line can explain the correlation between the \n",
    "dependent (y) and a single independent (x) variable. In the above\n",
    "figure, r<sup>2</sup> = 0.38, which means that 38% of the new-born babies could\n",
    "be explained by a linear correlation with the number of storks.\n",
    "\n",
    "In many cases, more than one independent variable is needed to explain\n",
    "the scatter in the data. In this case, Multiple Linear Regression is\n",
    "used where `y = x1 + x2 + x3….xn + b`. From this analysis\n",
    "(i.e. simultaneous solving for a system of linear equations – remember\n",
    "your Linear Algebra course!) the Multiple Coefficient of Determination\n",
    "(R<sup>2</sup>) is used to express the amount of explained variation in `y` by a\n",
    "combination of independent variables. By default, many programs use\n",
    "the R<sup>2</sup> number even when there is only one independent variable. This\n",
    "can be misleading as capital R<sup>2</sup> should be reserved for analyses that\n",
    "involve multiple independent variables.\n",
    "\n",
    "From a user perspective, we are interested to understand how good the\n",
    "model is, and how to interpret the key indicators of a given\n",
    "regression model:\n",
    "\n",
    "-   **r<sup>2</sup>:** or the coefficient of determination.  This value is in the range from zero to one and\n",
    "    expresses how much of the observed variance  in the data is explained by the regression\n",
    "    model. So a value of r<sup>2</sup>=0.7 indicates that 70% of the variance is\n",
    "    explained by the model, and that 30% of the variance is explained\n",
    "    by other processes which are not captured by the linear model\n",
    "    (e.g., measurements errors, or some non-linear effect affecting `x`\n",
    "    and `y`). In Fig. [BROKEN LINK: fig:storks] 38% of the variance in the birthrate\n",
    "    can be explained by the increase in stork pairs.\n",
    "-   **p:** When you do a linear regression, you state the\n",
    "    hypothesis that `y` depends on `x` and that they are linked by a\n",
    "    linear equation. If you test a hypothesis, you however also have to\n",
    "    test the so-called **null-hypothesis**, which in this case would\n",
    "     state that `y` is\n",
    "     unrelated to `x`. The p-value\n",
    "    expresses the likelihood that the null-hypothesis is true. So a\n",
    "    p-value of 0.1 indicates a 10% chance that your data does not\n",
    "    correlate. A p-value of 0.01, indicates a 1% chance that your data\n",
    "    is not correlated. Typically, we can reject the null-hypothesis if\n",
    "    `p < 0.05`, in other words, we are 95% sure the null hypothesis is\n",
    "    wrong. In Fig. [BROKEN LINK: fig:storks], we are 99.2% sure the null hypothesis is\n",
    "    wrong. Note that there is not always a simple relationship between\n",
    "    r<sup>2</sup> and p.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The statsmodel library\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Python's success rests to a considerable degree on the myriad of third\n",
    "party libraries which, unlike MatLab, are typically free to use. In\n",
    "the following, we will use the \"statsmodel\" library, but there are\n",
    "plenty of other statistical libraries we could use as well. \n",
    "\n",
    "The statsmodel library provides a few different interfaces. Here we will use\n",
    "the formula interface, which is similar to the R-formula\n",
    "syntax. However, not all statsmodel functions are available through\n",
    "this interface (yet?). First, we import the needed libraries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # import pandas as pd\n",
    "import pathlib as pl\n",
    "\n",
    "# define the file and sheetname we want to read. Note that the file\n",
    "# has to be present in the local working directory!\n",
    "fn: str = \"storks_vs_birth_rate.csv\"  # file name\n",
    "cwd: pl.Path = pl.Path.cwd()\n",
    "fqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\n",
    "if not fqfn.exists():  # check if the file is actually there\n",
    "    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n",
    "\n",
    "df: pd.DataFrame = pd.read_csv(fn)  # read data\n",
    "df.columns = [\"Babies\", \"Storks\"]  # replace colum names\n",
    "df.head()  # test that all went well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we perform a regression analysis, we must test if the data follows a normal\n",
    "distribution. There are a variety of tests to check for normality, but\n",
    "here we will simply use a histogram plot which either shows a bell\n",
    "curve distribution or not \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig: plt.Figure\n",
    "ax1: plt.Axes\n",
    "ax2: plt.Axes\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(nrows=2, ncols=1)  #\n",
    "fig.set_size_inches(6, 9)\n",
    "ax1.hist(\n",
    "    df.iloc[:, 0],\n",
    ")\n",
    "ax2.hist(df.iloc[:, 1])\n",
    "ax1.set_title(\"Babies\")\n",
    "ax2.set_title(\"Storks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the histogram, our data shows anything but a\n",
    "normal distribution! We will use it anyway since it is a fun\n",
    "dataset. However, the above test is crucial if you ever want to do a\n",
    "real regression analysis!\n",
    "\n",
    "For the regression model, we want to analyze whether the number of\n",
    "storks predict the number of babies. In other words, does the birth\n",
    "rate depend on the number of storks? For this, we need to define a\n",
    "statistical model, and test whether the model predictions will fit the\n",
    "data:\n",
    "\n",
    "-   The gory details of this procedure are beyond the scope of this\n",
    "    course - if you have not yet taken a stats class, I do recommend\n",
    "    doing so!\n",
    "-   There are many ways of doing this. Here we use an approach which\n",
    "    is common in `R`\n",
    "\n",
    "As of November 2021, there is no type hinting support for the statsmodel library.  However, to distinguish between the various statsmodel object types, I use the following hints:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smf.ols   ordinary least square model object\n",
    "# smf.ols.fit  model results object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below how this is used in code.\n",
    "\n",
    "After importing the data, we now create a statistical model on line 16\n",
    "in the code below.  Pay attention to how the model is specified with the\n",
    "formula `\"Babies ~ Storks`, which states that the number of Babies\n",
    "should depend on the number of storks. These names must correspond to\n",
    "the variable names in the data frame `df`!\n",
    "\n",
    "Once the model is defined, we request to fit the model against the data (line 17)\n",
    "The results of the `fit()` method will be stored in the `results` variable.\n",
    "Line 18, then  invokes the\n",
    "`summary()` method of the results object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd  # import pandas as pd\n",
    "import pathlib as pl\n",
    "\n",
    "# define the file and sheetname we want to read. Note that the file\n",
    "# has to be present in the local working directory!\n",
    "fn: str = \"storks_vs_birth_rate.csv\"  # file name\n",
    "cwd: pl.Path = pl.Path.cwd()\n",
    "fqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\n",
    "if not fqfn.exists():  # check if the file is actually there\n",
    "    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n",
    "\n",
    "df: pd.DataFrame = pd.read_csv(fn)  # read data\n",
    "df.columns = [\"Babies\", \"Storks\"]  # replace colum names\n",
    "\n",
    "model: smf.ols = smf.ols(formula=\"Babies ~ Storks\", data=df)\n",
    "results: model.fit = model.fit()  # fit the model to the data\n",
    "print(results.summary())  # print the results of the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plenty of information here, probably more than you asked for. \n",
    "Let's tease out the important ones:\n",
    "\n",
    "-   The first line states that `Babies` is the dependent variable. This\n",
    "    is useful and will help you to catch errors in your model\n",
    "    definition.\n",
    "-   The second line confirms that this is an ordinary least squares model\n",
    "-   Then, there are also a couple of warnings, indicating that your\n",
    "    data quality may be less than excellent. But we knew this already\n",
    "    from testing whether the data is normal distributed or not.\n",
    "\n",
    "If you compare the output with Figure [fig:storks](fig:storks), you can see that\n",
    "r<sup>2</sup> value is called \"R-squared\", the p-value is called \"Prob\n",
    "(F-statistic)\", the y-intercept is the first value in the \"Intercept\"\n",
    "row, the slope is the first value in the \"Storks\" row. You can also\n",
    "extract these parameters from the model results object like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve values from the model results\n",
    "slope   :float = results.params[1]  # the slope\n",
    "y0      :float = results.params[0]  # the y-intercept\n",
    "rsquare :float = results.rsquared   # rsquare\n",
    "pvalue  :float = results.pvalues[1] # the pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the regression line and confidence intervals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r<sup>2</sup> and p-value give us some indication of how well our regression\n",
    "model performs. However, we can add further information to our graph:\n",
    "\n",
    "-   The line which represents the regression model\n",
    "-   The confidence intervals that indicate the confidence we have in our\n",
    "    regression model.\n",
    "-   The confidence intervals that indicate the confidence we have in\n",
    "    the predictions we make based on our regression model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing the confidence interval data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not easy to find out how to access these parameters in the\n",
    "statslib library. So best to keep this code snippet in your template\n",
    "collection. Bottom line is, we will use the `summary_table()` function\n",
    "provided by the `statsmodels.stats.outliers_influence` module. We can\n",
    "then feed the `results` object of the regression analysis to this\n",
    "function, and it will return all sorts of data (most of which we can\n",
    "ignore). Note that the `alpha` keyword specifies the significance\n",
    "level for the confidence intervals we aim to retrieve. It is customary\n",
    "to provide this number as 1-significance (e.g., &alpha;=0.05 implies 1-&alpha; = 95%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "\n",
    "sig: float = 0.05  # = 1 - sig > 0.95 = 95% significance\n",
    "\n",
    "# variable types for the return values from summary_table\n",
    "st: SimpleTable  # table with results that can be printed\n",
    "data: list  # calculated measures and statistics for the table\n",
    "ss2: list[str]  # column_names for table (Note: rows of table are observations)\n",
    "\n",
    "st, data, ss2 = summary_table(results, alpha=0.05)\n",
    "\n",
    "# extract the data for predicted values and confidence intervals\n",
    "fitted_values: np.ndarray = data[:, 2]  # the regression line\n",
    "\n",
    "# confidence intervals for the model\n",
    "model_ci_low :np.ndarray # lower confidence limits\n",
    "model_ci_upp :np.ndarray # upper confidence limits\n",
    "model_ci_low, model_ci_upp = data[:, 4:6].T  # don't ask....\n",
    "\n",
    "# confidence intervals for the model predictions\n",
    "predict_mean_ci_low :np.ndarray\n",
    "predict_mean_ci_upp :np.ndarray\n",
    "predict_mean_ci_low, predict_mean_ci_upp = data[:, 6:8].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the confidence interval data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper and lower confidence boundaries describe the upper and lower\n",
    "boundaries of an area. We can either plot these boundaries as a line plot or as a shaded area.\n",
    "Matplotlib provides the `fill_between` method to shade the area between two lines:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax.fill_between(storks, model_ci_low, model_ci_up, alpha=0.1, color=\"C1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of the `alpha` keyword. This has nothing to do with the\n",
    "alpha which is used in statistics. Rather, it describes the\n",
    "transparency of the object you are drawing. If you set it to one (the\n",
    "default), the object will be fully opaque. If you set it to zero, it\n",
    "will be fully transparent (so you won't see it).  See the code below for an actual example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Stork Figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put it all together. Note that when we draw the figure, it\n",
    "matters whether we draw the confidence intervals first or\n",
    "last. Change the order in the code below, to see the difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pandas as pd  # inport pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib as pl\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "\n",
    "fn: str = \"storks_vs_birth_rate.csv\"  # file name\n",
    "cwd: pl.Path = pl.Path.cwd()\n",
    "fqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\n",
    "\n",
    "# test if file is present\n",
    "if not fqfn.exists():  # check if the file is actually there\n",
    "    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n",
    "\n",
    "# read data\n",
    "df: pd.DataFrame = pd.read_csv(fn)  # read data\n",
    "df.columns = [\"Babies\", \"Storks\"]  # replace colum names\n",
    "storks: pd.Series = df[\"Storks\"]\n",
    "babies: pd.Series = df[\"Babies\"]\n",
    "\n",
    "\n",
    "# ------ create linear regression model ------\n",
    "model: smf.ols = smf.ols(formula=\"Babies ~ Storks\", data=df)\n",
    "results: model.fit = model.fit()  # fit the model to the data\n",
    "\n",
    "\n",
    "# ------ extract model parameters\n",
    "slope: float = results.params[1]  # the slope\n",
    "y0: float = results.params[0]  # the y-intercept\n",
    "rsquare: float = results.rsquared  # rsquare\n",
    "pvalue: float = results.pvalues[1]  # the pvalue\n",
    "ds: str = (\n",
    "    f\"y = {y0:1.4f}+x*{slope:1.4f}\\n\" f\"$r^2$ = {rsquare:1.2f}\\n\" f\"p = {pvalue:1.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------ extract confidence intervals ------\n",
    "sig: float = 0.05  # = 1 - sig > 0.95 = 95% significance\n",
    "\n",
    "# variable types for the return values from summary_table\n",
    "st: SimpleTable  # table with results that can be printed\n",
    "data: np.ndarray  # calculated measures and statistics for the table\n",
    "ss2: list[str]  # column_names for table (Note: rows of table are observations)\n",
    "\n",
    "# variable types for the confidence interval data\n",
    "model_ci_low: np.ndarray\n",
    "model_ci_up: np.ndarray\n",
    "predict_mean_ci_low: np.ndarray\n",
    "predict_mean_ci_up: np.ndarray\n",
    "\n",
    "# get data\n",
    "st, data, ss2 = summary_table(results, alpha=sig)\n",
    "\n",
    "# extract regression line\n",
    "fitted_values: np.ndarray = data[:, 2]\n",
    "\n",
    "# extract confidence intervals for the model\n",
    "model_ci_low, model_ci_up = data[:, 4:6].T  #\n",
    "\n",
    "# extract confidence intervals for the predictions\n",
    "predict_mean_ci_low, predict_mean_ci_up = data[:, 6:8].T\n",
    "\n",
    "# ------ create plot ------\n",
    "fig: plt.Figure  # this variable  will hold the canvas object\n",
    "ax: plt.Axes  # this variable will hold the axis object\n",
    "fig, ax = plt.subplots()  # create canvas and axis objects\n",
    "\n",
    "# plot confidence intervals first\n",
    "ax.fill_between(storks, predict_mean_ci_low, predict_mean_ci_up, alpha=0.1, color=\"C1\")\n",
    "ax.fill_between(storks, model_ci_low, model_ci_up, alpha=0.2, color=\"C1\")\n",
    "\n",
    "# add data points\n",
    "ax.scatter(storks, babies, color=\"C0\")\n",
    "\n",
    "# regression line\n",
    "ax.plot(storks, fitted_values, color=\"C1\")\n",
    "\n",
    "# plot options and annotations\n",
    "plt.style.use(\"ggplot\")\n",
    "fig.set_size_inches(6, 4)\n",
    "fig.set_dpi(120)\n",
    "\n",
    "ax.text(1000, 1750, ds, verticalalignment=\"top\")\n",
    "ax.set_xlabel(\"Stork Pairs\")\n",
    "ax.set_ylabel(\"Newborn Babies [$10^3$]\")\n",
    "fig.set_tight_layout(\"tight\")\n",
    "fig.savefig(\"stork_new.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is quite lengthy, especially given the fact that\n",
    "you can do a regression analysis with a few clicks in excel. On the\n",
    "other hand, if you re-arrange the code a little bit, and use generic\n",
    "variables, you can create a code template where you only specify a few\n",
    "key parameters at the beginning of the code, and then you can\n",
    "generate a much more meaningful regression analysis with a few keystrokes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Description:\n",
    "    Author:\n",
    "    Date:\n",
    "\"\"\"\n",
    "# ----------- third party library imports ------------------\n",
    "from __future__ import annotations\n",
    "import pandas as pd  # inport pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib as pl\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "\n",
    "# ----------- user serviceable pararameters\n",
    "data_file: str = \"\"  # csv file name\n",
    "figure_name: str = \"\"  # figure name\n",
    "\n",
    "x_var_loc: int = 0  # colum # in spreadsheet\n",
    "y_var_loc: int = 1  # colum # in spreadsheer\n",
    "\n",
    "x_axis_label: str = \"\"\n",
    "y_axis_label: str = \"\"\n",
    "size_x: number = 6  # size in inches\n",
    "size_y: number = 4  # size in inches\n",
    "\n",
    "confidence_level: float = 0.05  # 1 - alpha in %\n",
    "\n",
    "# ----------- main program ---------------------------------\n",
    "# --- variable declarations\n",
    "\n",
    "# --- code starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Robert Matthews, Storks Devilver Babies (p = 0.008), Teaching\n",
    "    Statistics 22:2, p 36-38, 2000,\n",
    "    [https://doi.org/10.1111/1467-9639.00013](https://doi.org/10.1111/1467-9639.00013)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
