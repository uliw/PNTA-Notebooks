{"cells":[{"cell_type":"markdown","id":"8a6523c4-a88e-497d-aff9-1aa9663745cf","metadata":{},"source":["## Linear Regression\n\n"]},{"cell_type":"markdown","id":"a21b62af-ae29-4493-a8b6-681dcfab1dcc","metadata":{},"source":["While Excel and many Python plotting libraries offer straightforward methods to plot a regression line, conducting a thorough analysis of linear regression is actually quite complex. In this module, we will demonstrate the proper approach to linear regression analysis. Specifically, we will examine whether the data is appropriate for this type of analysis, how to visualize confidence intervals, and, as a valuable addition, how to plot the confidence intervals for predictions derived from the linear regression model.\n\n"]},{"cell_type":"markdown","id":"aff7ed4c-1249-4be1-9209-200ac56e212a","metadata":{},"source":["### Causation versus Correlation\n\n"]},{"cell_type":"markdown","id":"27061e2c-ff17-4502-ac36-90ef70996fb2","metadata":{},"source":["Back in my home country, and before the hippy movement changed our culture, kids, who were curious about where the babies come from, were told that they are brought by the stork (a large bird, see Fig.[ref:fig:storksa](ref:fig:storksa)). Storks were indeed a common sight in rural areas, and large enough to sell this story to a 3-year-old.\n\n![img](Ringedwhitestork.jpg \"The Stork. Image by Soloneying, Wikimedia Downloaded Nov 22, 2019.\")\n\nAs mature scientists who value critical thinking, we seek data to support this idea. Specifically, we expect to find a strong correlation between the number of storks and the number of babies born. Surprisingly, these two variables do indeed correlate in a statistically significant manner. Countries with larger stork populations tend to have higher birthrates. This phenomenon, where both variables increase together, is referred to as a positive correlation.. See Fig. [4](#org78fe66b)\n\n![img](./stork_new.png \"The birthrate and the number of stork pairs correlate in a statistically significant way. This analysis suggests that each stork pair delivers about 29 human babies, and that about 225 thousand babies were born otherwise. Data after Matthews 2000.\")\n\nThis does not prove that storks deliver babies. Clearly, the correlation between two observable quantities does not imply causation. A more plausible explanation is that both variables are influenced by a shared factor, such as industrialization. It is a common mistake to confuse correlation with causation. Another good example is to correlate drinking with heart attacks. This surely will correlate, but the story is more complex. Are there, e.g., patterns like drinkers tend to do less exercise than non-drinkers? So even if you have a good hypothesis why two variables are correlated, the correlation itself proves nothing.\n\n \nRegardless of a causal relationship, the correlation between two datasets can be quantified using the Pearson Product Moment Correlation Coefficient (PPMCC, commonly referred to as r). This coefficient indicates both the strength and direction of the relationship between two variables. The PPMCC ranges from +1 (indicating a perfect positive correlation) to -1 (indicating a perfect negative or inverse correlation). Correlations are considered weak when they fall between +0.3 and -0.3, and strong when they exceed +0.7 or are lower than -0.7. It is important to note that correlation analysis does not assume any specific functional form between the dependent variable (y) and the independent variable (x). This means that the PPMCC does not provide information on whether the correlation is linear, logarithmic, exponential, or of any other form.\nWe can use the `corr()` method of the pandas series object to calculate the PPMCC:\n\n"]},{"cell_type":"code","execution_count":1,"id":"67d02ff0-0df2-4860-9667-9931cc32fe45","metadata":{},"outputs":[],"source":["import pandas as pd  # inport pandas as pd\nimport pathlib as pl\n\nfn: str = \"storks_vs_birth_rate.csv\"  # file name\ncwd: pl.Path = pl.Path.cwd()\nfqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\n\nif not fqfn.exists():\n    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n\ndf: pd.DataFrame = pd.read_csv(fn)  # read data\ndf.columns = [\"Babies\", \"Storks\"]  # replace column names \nb: pd.Series = df[\"Babies\"]\ns: pd.Series = df[\"Storks\"]\n\nprint(f\" r = {s.corr(b):.2f}\")"]},{"cell_type":"markdown","id":"7a7427bb-4577-4b74-a4bf-2f13bbf8c48c","metadata":{},"source":["Based on this analysis, we can confirm that the number of babies and storks correlate.\n\n"]},{"cell_type":"markdown","id":"b95350bf-1f3b-4c1f-929f-21373433b2ab","metadata":{},"source":["### Understanding the results of a linear regression analysis\n\n"]},{"cell_type":"markdown","id":"487ca0ad-d140-4009-a8d2-e45a3f08aae2","metadata":{},"source":["Linear regression analysis builds upon correlation analysis by evaluating how effectively a linear function, such as a straight line, represents the relationship between two variables, x and y. A straight line is described by the equation `y = mx + b`, where adjustments are made to the coefficients m (slope) and b (y-intercept) to minimize the difference between the observed data points and the predictions made by the model. Numerous libraries are available to facilitate this fitting process, and ideally, they provide not only the resulting linear model (the equation of the fitted line) but also additional parameters that assess the quality of the fit.\n\nThe most common parameter is the coefficient of determination, denoted as r<sup>2</sup>. This metric indicates how effectively a linear relationship can explain the correlation between the dependent variable `y` and a single independent variable `x`. In the figure above, r<sup>2</sup> = 0.38, suggesting that 38% of the variation in the number of new-born babies can be accounted for by a linear correlation with the number of storks. Often, however, multiple independent variables are needed to better capture the variability in the data. In such cases, Multiple Linear Regression is utilized, represented as `y = x1 + x2 + x3… + xn + b`. This approach involves simultaneously solving a system of linear equations (a concept from Linear Algebra) and provides the Multiple Coefficient of Determination, denoted as R<sup>2</sup>, which quantifies the proportion of variability in `y` explained by the combination of independent variables x<sub>i</sub>. Many programs default to using R<sup>2</sup> even with a single independent variable, which can be misleading since capital R<sup>2</sup> should be reserved for analyses that involve multiple independent variables.\n\nFrom a user perspective, we are interested to understand how good the model is, and how to interpret the key indicators of a given regression model:\n\n-   **r<sup>2</sup>:** or the coefficient of determination. This value is in the range from zero to one and expresses how much of the observed variance in the data is explained by the regression model. So a value of r<sup>2</sup>=0.7 indicates that 70% of the variance is explained by the model, and that 30% of the variance is explained by other processes which are not captured by the linear model (e.g., measurements errors, or some non-linear effect affecting `x` and `y`). In Fig. [BROKEN LINK: fig:storks] 38% of the variance in the birthrate can be explained by the increase in stork pairs.\n-   **p:** In linear regression, we hypothesize that `y` depends on `x` and that they are connected by a linear equation. When testing a hypothesis, it is essential to also evaluate the **null hypothesis**, which in this context asserts that `y` is unrelated to `x`. The p-value indicates the probability that the null hypothesis is true. For instance, a p-value of 0.1 suggests a 10% chance that there is no correlation in your data, while a p-value of 0.01 indicates a 1% chance of no correlation. Generally, we can reject the null hypothesis if `p < 0.05`, implying that we are 95% confident the null hypothesis is incorrect. In Fig. [BROKEN LINK: fig:storks], we achieve a confidence level of 99.2% in deeming the null hypothesis false. It is important to note that the relationship between r² and p is not always straightforward.\n\n"]},{"cell_type":"markdown","id":"f39248bf-0f7a-4139-8d9c-d785bf15652e","metadata":{},"source":["### Testing the data\n\n"]},{"cell_type":"markdown","id":"d03d6df3-7c58-488d-b114-af40226dc0de","metadata":{},"source":["#### Histograms\n\n"]},{"cell_type":"markdown","id":"b5681a7d-16e2-4883-8426-5fc607a38df3","metadata":{},"source":["\nThe Storks data set is interesting; however, it does not meet a crucial requirement for linear regression analysis: the data must exhibit a [Gaussian Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution) (or bell curve). \nTo quickly assess the distribution of your data, you can create a [histogram plot](https://en.wikipedia.org/wiki/Histogram) to determine whether the x and y values are normally distributed.\n\n"]},{"cell_type":"code","execution_count":1,"id":"379b8a04-b340-4368-97c9-d224a44f9324","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"|   | Babies | Storks |\n|---+--------+--------|\n| 0 |    118 |      1 |\n| 1 |    188 |      4 |\n| 2 |    551 |      5 |\n| 3 |     59 |      9 |\n| 4 |     83 |    100 |"}],"source":["import matplotlib.pyplot as plt\nimport pandas as pd  # import pandas as pd\nimport pathlib as pl\n\nfn: str = \"storks_vs_birth_rate.csv\"  # file name\ncwd: pl.Path = pl.Path.cwd()\nfqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\nif not fqfn.exists():  # check if the file is actually there\n    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n\ndf: pd.DataFrame = pd.read_csv(fn)  # read data\ndf.columns = [\"Babies\", \"Storks\"]  # replace colum names\ndisplay(df.head())  # test that all went well\n\nfig, [ax1, ax2] = plt.subplots(nrows=2, ncols=1)  #\nax1.hist(\n    df.iloc[:, 0],\n)\nax2.hist(df.iloc[:, 1])\nax1.set_title(\"Babies\")\nax2.set_title(\"Storks\")\nplt.tight_layout()\nplt.savefig(\"strk_histogram.png\")\nplt.show()"]},{"cell_type":"markdown","id":"bbeac349-0ed2-44ef-93db-6332dc6d033a","metadata":{},"source":["![img](strk_histogram.png)\nAs shown in the histogram, our data clearly does not follow a normal distribution. Nevertheless, we will proceed with it, as it is an interesting dataset. However, conducting the aforementioned test is essential if you plan to perform a thorough regression analysis in the future!\n\n"]},{"cell_type":"markdown","id":"e7a97d0c-8746-4d77-907d-1e0051a3a335","metadata":{},"source":["#### Qqplots\n\n"]},{"cell_type":"markdown","id":"c604e797-0ac4-4dd6-9050-33f9c1cea109","metadata":{},"source":["\nAnother method to assess normality is by using a [QQ plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot), which determines if your data follows a normal distribution. However, the statsmodels QQ plot function does not adhere to the standard matplotlib syntax, making it challenging to customize colors. \n\n"]},{"cell_type":"code","execution_count":1,"id":"c86fd243-b6da-4dd9-a267-e7fc9f574f0f","metadata":{},"outputs":[],"source":["import statsmodels.api as sm\n\nfig, ax = plt.subplots()\nsm.qqplot(df['Babies'], ax=ax, line=\"s\")\nplt.savefig(\"qqplot.png\")\nplt.show()"]},{"cell_type":"markdown","id":"bf928ac8-0677-486f-819a-261c994c70c3","metadata":{},"source":["![img](qqplot.png)\nHere's how to interpret a QQ plot:\n\n1.  **Axes**: \n    -   The x-axis represents the theoretical quantiles from the expected distribution (often a normal distribution).\n    -   The y-axis represents the quantiles from the observed dataset.\n\n2.  **Points**: \n    -   Each point on the plot corresponds to a quantile of the observed data plotted against a quantile from the theoretical distribution.\n    -   If the dataset follows the theoretical distribution closely, the points will lie along a straight line (often the 45-degree line).\n\n3.  **Line of Identity**: \n    -   A reference line (y = x) is typically drawn, representing where points would lie if the two distributions were identical.\n    -   Deviations from this line indicate differences between the two distributions.\n\n4.  **Interpreting Patterns**:\n    -   **Straight Line**: If the points lie close to the line, it suggests that the observed data follows the theoretical distribution well.\n    -   **Curve or S-Curve**: If the points curve away from the line (upward or downward), it indicates that the tails of the observed distribution are heavier or lighter than the theoretical distribution.\n    -   **Outliers**: Points that are far from the line can indicate potential outliers in the data.\n\n5.  **Specific Cases**:\n    -   **Left Skewed Distribution**: Points tend to be above the line for lower quantiles and below for higher quantiles.\n    -   **Right Skewed Distribution**: Points tend to be below the line for lower quantiles and above for higher quantiles.\n    -   **Normal Distribution**: For normally distributed data, the QQ plot should closely resemble a straight line.\n\nBy analyzing these aspects, you can see that the stork data is not normal distributed.\n\n"]},{"cell_type":"markdown","id":"4d30b1ea-656f-4e05-99ce-b0c72da6d99b","metadata":{},"source":["#### The Shapiro-Wilk Test\n\n"]},{"cell_type":"markdown","id":"846e2bde-474a-4382-9a89-e77488387a4f","metadata":{},"source":["The tests mentioned above depend on visual interpretation. The [Shapiro-Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) offers a systematic approach to assess normal distribution. The following snippet illustrates how to implement and interpret the test.\n\n"]},{"cell_type":"code","execution_count":1,"id":"f768d48b-3038-45f8-9686-08b4d8eac108","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Shapiro Wilk Test Statistics W = 0.615, p = 0.000\nSample likely not Gaussian"}],"source":["from scipy.stats import shapiro\n\n# Do a Shapiro Wilk test\nstat, p = shapiro(df['Storks'])\nprint(f\"Shapiro Wilk Test Statistics W = {stat:.3f}, p = {p:.3f}\")\n\n# interpretation\nif p > 0.05 and stat > 0.8:\n    print(\"Sample looks Gaussian\")\nelif p <= 0.05 and stat > 0.8:\n    print(\"Sample might be Gaussian\")\nelif p <= 0.05 and stat <= 0.8:\n    print(\"Sample likely not Gaussian\")"]},{"cell_type":"markdown","id":"ffdcae1f-3aef-48ec-b9f0-84ff0e8e9330","metadata":{},"source":["Here's how to interpret the results of the test:\n\n1.  **Null Hypothesis (H0)**: The null hypothesis states that the data follows a normal distribution.\n\n2.  **Alternative Hypothesis (H1)**: The alternative hypothesis states that the data does not follow a normal distribution.\n\n3.  **Test Statistic (W)**: The test calculates a test statistic (W) that compares the observed distribution of the data to a normal distribution. A W value close to 1 indicates that the data may be normally distributed.\n\n4.  **p-value**: The p-value is a crucial output of the Shapiro-Wilk test. It indicates the probability of observing the test statistic under the null hypothesis.\n    -   A **high p-value** (typically greater than 0.05) suggests that you fail to reject the null hypothesis, implying that there is not enough evidence to conclude that the data is not normally distributed.\n    -   A **low p-value** (typically less than or equal to 0.05) suggests that you reject the null hypothesis, indicating that the data does not follow a normal distribution.\n\n5.  **Sample Size Considerations**: \n    -   The Shapiro-Wilk test is more powerful with small sample sizes. With larger samples, even minor deviations from normality can lead to significant p-values.\n\n6.  **Practical Implications**: If your dataset is determined to be significantly different from normal (p-value less than 0.05), you may consider using non-parametric statistical methods for further analysis, as many statistical tests assume normality.\n\nIn summary, the key to interpreting the Shapiro-Wilk test lies in examining the p-value in relation to your chosen significance level (commonly 0.05), which helps you determine whether your data can be considered normally distributed or not.\n\n"]},{"cell_type":"markdown","id":"74f78449-c7d7-4f70-afae-2a10bc57a1c4","metadata":{},"source":["### The statsmodels library\n\n"]},{"cell_type":"markdown","id":"f9aff09f-5600-42b6-a878-f04c69943b05","metadata":{},"source":["Python's success largely stems from its vast array of third-party libraries, which are generally free to use, in contrast to Matlab. In this section, we will utilize the \"statsmodels\" library, though many other statistical libraries are also available. The statsmodels library offers several interfaces, and we will focus on the formula interface, which resembles R's formula syntax (for more details, see [https://www.statsmodels.org/dev/example_formulas.html](https://www.statsmodels.org/dev/example_formulas.html)). However, it's important to note that not all functions in statsmodels are accessible through this interface at this time.\n\nFor the regression model, we aim to investigate whether the number of storks correlates with the number of babies born. In other words, does the birth rate depend on the stork population? To do this, we must define a statistical model and evaluate how well its predictions align with the data. **It is important to note that as of March 2025, the statsmodels library does not support type hinting.** \n\nTo enhance clarity, we will replace the specific data-dependent names in the lengthy code with more general aliases. For the independent data (\"Storks\"), we will use `X`, and for the dependent data (\"Babies\"), we will use `Y`. I  use the `df.columns()` method (line 12) to set the column names to `\"X\"` and `\"Y`. This approach allows us to avoid having to change variable names in multiple locations if we repeat this process in the future. After importing the data, we proceed to create a statistical model on line 16 of the code below. Note how the model is specified using the formula `\"Y\" ~ \"X\"` indicating that the number of Babies (`Y`)is dependent on the number of Storks (`X`). It is essential that these names match the variable names in the data frame `df`. Once the model is defined, we call the `fit()` method to fit the model using the data (line 17). The results of this method will be stored in the `results` variable. Line 18 subsequently invokes the `summary()` method on the results object.\n\n"]},{"cell_type":"code","execution_count":1,"id":"01163cd1-eb7d-45de-962c-00f8e11661f6","metadata":{},"outputs":[],"source":["import statsmodels.formula.api as smf\nimport pandas as pd  # import pandas as pd\nimport pathlib as pl\n\nfn: str = \"storks_vs_birth_rate.csv\"  # file name\ncwd: pl.Path = pl.Path.cwd()\nfqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\nif not fqfn.exists():  # check if the file is actually there\n    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n\ndf: pd.DataFrame = pd.read_csv(fn)  # read data\ndf.columns = [\"Y\", \"X\"]  # replace colum names\n\nmodel = smf.ols(formula=\"Y ~ X\", data=df) # create model\nresults = model.fit()  # fit the model to the data\ndisplay(results.summary())  # print the results of the analysis"]},{"cell_type":"markdown","id":"f63b0d9e-dd12-4757-8136-5d188c61558f","metadata":{},"source":["Plenty of information here, probably more than you asked for. Let's tease out the important ones:\n\n-   The first line states that `Babies` is the dependent variable. This is useful and will help you to catch errors in your model definition.\n-   The second line confirms that this is an ordinary least squares model\n-   Then, there are also a couple of warnings, indicating that your data quality may be less than excellent. But we knew this already from testing whether the data is normal distributed or not.\n\nIf you compare the output with Figure [ref:fig:storks](ref:fig:storks), you can see that r<sup>2</sup> value is called \"R-squared\", the p-value is called \"Prob (F-statistic)\", the y-intercept is the first value in the \"Intercept\" row, the slope is the first value in the \"Storks\" row. You can also extract these parameters from the model results object like this:\n\n"]},{"cell_type":"code","execution_count":1,"id":"d82c9785-2971-44d5-93b5-b92f30984b61","metadata":{},"outputs":[],"source":["\"\"\" Retrieve parameters from the model results. Note\nthat the dictionary key 'x' must be equal to the\nname of the independent variable used in the model\ndefinition (i.e., y~x)\n\"\"\"\n\nslope: float = results.params[\"X\"]  # the slope\ny_0: float = results.params[\"Intercept\"]  # the y-intercept\nr_square: float = results.rsquared  # rsquare\np_value: float = results.pvalues[\"X\"]  # the p-value"]},{"cell_type":"markdown","id":"70e9a2b4-2c49-408b-a070-1607f7ac93dd","metadata":{},"source":["### Adding the regression line and confidence intervals\n\n"]},{"cell_type":"markdown","id":"89a36588-a52c-404e-a877-bf4623a6eb02","metadata":{},"source":["The r<sup>2</sup> and p-value give us some indication of how well our regression model performs. However, we can add further information to our graph:\n\n-   The line which represents the regression model\n-   The confidence intervals that indicate the confidence we have in our\n    regression model.\n-   The confidence intervals that indicate the confidence we have in the\n    predictions we make based on our regression model\n\n"]},{"cell_type":"markdown","id":"074bb76d-0e13-4982-8171-52a3a46874c5","metadata":{},"source":["#### Accessing the confidence interval data\n\n"]},{"cell_type":"markdown","id":"33b40a0a-b0ca-4b48-a349-dc9b680dc998","metadata":{},"source":["Confidence intervals visually illustrate the reliability of your linear model. There are two types of confidence intervals: A) the confidence that your linear model accurately represents the data—meaning that any regression line within this interval is likely valid; and B) the confidence in the predictions derived from your model. This second interval is often more intriguing, yet commercial software that offers this information is scarce. A crucial parameter related to confidence intervals is the level of significance . This level indicates how confident you wish to be, such as 99%, 95%, or 70% certain that you are right. Statistical software typically designates the significance level with the alpha keyword, and it is customary to express this value as 1 minus the significance (for example, &alpha; = 0.05 corresponds to a 95% confidence level).\n\nWe can retrieve this information from our model results object in the following way:\n\n"]},{"cell_type":"code","execution_count":1,"id":"32e6761e-7355-4fc7-9def-cca7e33f3efc","metadata":{},"outputs":[],"source":["prediction_data = results.get_prediction(df.X) # extract prediction data\npred_summary = pred.summary_frame(alpha=0.05) # get summary at 95%\n\n# get the confidence interval for the model\nmodel_ci_low: NDArrayFloat = pred_summary['mean_ci_lower']\nmodel_ci_upp: NDArrayFloat = pred_summary['mean_ci_upper']\n\n# get the confidence interval for the prediction\npredict_ci_low: NDArrayFloat = pred_summary['obs_ci_lower']\npredict_ci_upp: NDArrayFloat = pred_summary['obs_ci_upper']"]},{"cell_type":"markdown","id":"08a26979-cd3d-4344-a9fb-9cd2fbf395e7","metadata":{},"source":["#### Plotting the confidence interval data\n\n"]},{"cell_type":"markdown","id":"60295b93-0419-4afa-9ea2-6cd6c0aab9f5","metadata":{},"source":["The upper and lower confidence boundaries describe the upper and lower boundaries of an area. We can either plot these boundaries as a line plot or as a shaded area.  Matplotlib provides the `fill_between` method @@latex:\\index{matplotlib!fill-between}to shade the area between two lines:\n\n"]},{"cell_type":"code","execution_count":1,"id":"f14ca255-3cbe-4925-b934-f418ddec302f","metadata":{},"outputs":[],"source":["ax.fill_between(storks, model_ci_low, model_ci_up, alpha=0.1, color=\"C1\")"]},{"cell_type":"markdown","id":"d7daa368-ae33-4aeb-9350-cb5f15165f73","metadata":{},"source":["Note the use of the `alpha` keyword. This has nothing to do with the alpha which is used in statistics. Rather, it describes the transparency of the object you are drawing. If you set it to one (the default), the object will be fully opaque. If you set it to zero, it will be fully transparent (so you won't see it).  See the code below for an actual example.\n\n"]},{"cell_type":"markdown","id":"1d91709c-cda8-43d8-8ea7-b115d0735155","metadata":{},"source":["### Creating the Stork Figure\n\n"]},{"cell_type":"markdown","id":"d56434a8-db48-437f-a098-c13a8caed51e","metadata":{},"source":["Now let's put it all together. Note that when we draw the figure, it matters whether we draw the confidence intervals first or last. Change the order in the code below, to see the difference.\n\n"]},{"cell_type":"code","execution_count":1,"id":"6c8829b1-5c2e-46a6-a876-b56a58ea2e19","metadata":{},"outputs":[],"source":["import numpy as np\nimport numpy.typing as npt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pathlib as pl\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.outliers_influence import summary_table\n\nfn: str = \"storks_vs_birth_rate.csv\"  # data file name\nfig_name: str = \"stork_new.png\"\nx_axis_label = \"Stork Pairs\"\ny_axis_label = \"Newborn Babies [$10^3$]\"\nsignificance = 0.05  # = 1 - sig > 0.95 = 95% significance\n\ncwd: pl.Path = pl.Path.cwd()\nfqfn: pl.Path = pl.Path(f\"{cwd}/{fn}\")\nif not fqfn.exists():  # check if the file is actually there\n    raise FileNotFoundError(f\"Cannot find file {fqfn}\")\n\ndf: pd.DataFrame = pd.read_csv(fn).dropna()  # read data\ndf.columns = [\"Y\", \"X\"]  # replace colum names\ndf.sort_values(by = \"X\", ascending=True)\n\n# ------ create linear regression model ------\nmodel: smf.ols = smf.ols(formula=\"Y ~ X\", data=df)\nresults: model.fit = model.fit()  # fit the model to the data\n\n# ------ extract model parameters\nfitted_values: NDArrayFloat = results.fittedvalues\nslope: float = results.params[\"X\"]  # the slope = name of y\ny_0: float = results.params[\"Intercept\"]  # the y-intercept\nr_square: float = results.rsquared  # r_square\np_value: float = results.pvalues[\"X\"]  # the p_value for y\nds: str = (\n    f\"y = {y_0:1.4f}+x*{slope:1.4f}\\n\"\n    f\"$r^2$ = {r_square:1.2f}\\n\"\n    f\"p = {p_value:1.4f}\"\n)\n\n# ------ extract confidence intervals ------\nprediction_data = results.get_prediction(df.X) # extract prediction data\npred_summary = pred.summary_frame(alpha=significance) # get summary at 95%\n\n# get the confidence interval for the model\nmodel_ci_low: NDArrayFloat = pred_summary['mean_ci_lower']\nmodel_ci_upp: NDArrayFloat = pred_summary['mean_ci_upper']\n\n# get the confidence interval for the prediction\npredict_ci_low: NDArrayFloat = pred_summary['obs_ci_lower']\npredict_ci_upp: NDArrayFloat = pred_summary['obs_ci_upper']\nNDArrayFloat = npt.NDArray[np.float64]\n\n# ------ create plot ------\nfig, ax = plt.subplots()  # create canvas and axis objects\n\n# plot confidence intervals first\nax.fill_between(df.X, predict_ci_low, predict_ci_upp, alpha=0.1, color=\"C1\")\nax.fill_between(df.X, model_ci_low, model_ci_upp, alpha=0.2, color=\"C1\")\n# add data points\nax.scatter(df.X, df.Y, color=\"C0\")\n# regression line\nax.plot(df.X, fitted_values, color=\"C1\")\n# plot options and annotations\nplt.style.use(\"ggplot\")\nfig.set_size_inches(6, 4)\nfig.set_dpi(120)\nax.text(1000, 1750, ds, verticalalignment=\"top\")\nax.set_xlabel(x_axis_label)\nax.set_ylabel(y_axis_label)\nfig.set_tight_layout(\"tight\")\nfig.savefig(fig_name)\nplt.show()"]},{"cell_type":"markdown","id":"c3a7f516-0a70-45c5-ace7-4687dbfc944a","metadata":{},"source":["### References\n\n"]},{"cell_type":"markdown","id":"c8aa7198-dbb9-4ab5-b1e9-b7a3d5b57a32","metadata":{},"source":["-   Robert Matthews, 2000. Storks Devilver Babies (p = 0.008), Teaching Statistics, vol22:2, p. 36-38, [https://doi.org/10.1111/1467-9639.00013](https://doi.org/10.1111/1467-9639.00013),\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}